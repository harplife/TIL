{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words tutorial, part 2\n",
    "    \n",
    "    튜토리얼 part1에서 사용했던 BoW 대신,\n",
    "    Google에 Word2Vec 알고리즘을 사용하여 학습데이터를 구성.\n",
    "    Word2Vec은 \"Distributed Representation\"을 기반으로 한다.\n",
    "    Distributed Representation은 비지도 학습을 사용한 모델이다.\n",
    "    BOW는 데이터의 반복수 최상위 n개의 단어 Vocabulary,\n",
    "    vocabulary에 대한 각 문장의 단어들에 대한 빈도를 이용하여\n",
    "    feature vector를 만드는 것과 달리,\n",
    "    Distributed Represantation은 데이터셋 전체에 있는 단어들로\n",
    "    단어들 서로간의 연관관계를 구성한 것이다.\n",
    "    즉, 각 단어에 연관된 단어 세트가 구성된다.\n",
    "    (flower => plant, pretty, stem, leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/bow/labeledTrainData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"./data/bow/testData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"./data/bow/unlabeledTrainData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 => 단어 리스트 함수, 단어 리스트 => 문장 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 1. remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. convert words to lower case and split\n",
    "    words = review_text.lower().split()\n",
    "    # 4. optional: remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # 5. return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문단/단락 => 문장 => 단어 리스트\n",
    "# 1. nltk의 punkt tokenizer를 사용한다.\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# 2. 문단/단락을 문장으로 바꾸는 함수 생성\n",
    "def review_to_sents(review, tokenizer, remove_stopwords=False):\n",
    "    # a. use tokenizer\n",
    "    raw_sents = tokenizer.tokenize(review.strip())\n",
    "    # b. loop over each sentence\n",
    "    sentences = []\n",
    "    for sent in raw_sents:\n",
    "        # if a sent empty, skip\n",
    "        if len(sent) > 0:\n",
    "            # otherwise, use review_to_wordlist\n",
    "            sentences.append(review_to_wordlist(sent,remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sents(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sents(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maybe',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'eighties',\n",
       " 'just',\n",
       " 'to',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'up',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'is',\n",
       " 'guilty',\n",
       " 'or',\n",
       " 'innocent']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 300 # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-26 12:27:45,407 : INFO : collecting all words and their counts\n",
      "2019-03-26 12:27:45,408 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-26 12:27:45,461 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-03-26 12:27:45,510 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-03-26 12:27:45,556 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-26 12:27:45,605 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2019-03-26 12:27:45,652 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2019-03-26 12:27:45,698 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2019-03-26 12:27:45,745 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2019-03-26 12:27:45,793 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2019-03-26 12:27:45,840 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2019-03-26 12:27:45,887 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2019-03-26 12:27:45,933 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2019-03-26 12:27:45,981 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2019-03-26 12:27:46,029 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2019-03-26 12:27:46,073 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2019-03-26 12:27:46,121 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2019-03-26 12:27:46,168 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2019-03-26 12:27:46,216 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2019-03-26 12:27:46,264 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2019-03-26 12:27:46,312 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2019-03-26 12:27:46,360 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2019-03-26 12:27:46,408 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2019-03-26 12:27:46,455 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2019-03-26 12:27:46,506 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2019-03-26 12:27:46,556 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2019-03-26 12:27:46,604 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2019-03-26 12:27:46,653 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2019-03-26 12:27:46,702 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2019-03-26 12:27:46,750 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2019-03-26 12:27:46,796 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2019-03-26 12:27:46,850 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2019-03-26 12:27:46,904 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2019-03-26 12:27:46,956 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2019-03-26 12:27:47,008 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2019-03-26 12:27:47,055 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2019-03-26 12:27:47,104 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2019-03-26 12:27:47,153 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2019-03-26 12:27:47,208 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2019-03-26 12:27:47,258 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2019-03-26 12:27:47,310 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2019-03-26 12:27:47,356 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2019-03-26 12:27:47,404 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2019-03-26 12:27:47,452 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2019-03-26 12:27:47,501 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2019-03-26 12:27:47,552 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2019-03-26 12:27:47,599 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2019-03-26 12:27:47,647 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2019-03-26 12:27:47,696 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2019-03-26 12:27:47,744 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2019-03-26 12:27:47,793 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2019-03-26 12:27:47,841 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2019-03-26 12:27:47,890 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2019-03-26 12:27:47,937 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2019-03-26 12:27:47,986 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2019-03-26 12:27:48,036 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2019-03-26 12:27:48,087 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2019-03-26 12:27:48,136 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2019-03-26 12:27:48,188 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2019-03-26 12:27:48,236 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2019-03-26 12:27:48,284 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2019-03-26 12:27:48,333 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2019-03-26 12:27:48,383 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2019-03-26 12:27:48,435 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2019-03-26 12:27:48,484 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2019-03-26 12:27:48,533 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2019-03-26 12:27:48,583 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2019-03-26 12:27:48,630 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2019-03-26 12:27:48,678 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2019-03-26 12:27:48,728 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2019-03-26 12:27:48,776 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2019-03-26 12:27:48,826 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2019-03-26 12:27:48,873 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
      "2019-03-26 12:27:48,921 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
      "2019-03-26 12:27:48,970 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n",
      "2019-03-26 12:27:49,018 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2019-03-26 12:27:49,066 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-26 12:27:49,115 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2019-03-26 12:27:49,166 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2019-03-26 12:27:49,218 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2019-03-26 12:27:49,270 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2019-03-26 12:27:49,298 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2019-03-26 12:27:49,298 : INFO : Loading a fresh vocabulary\n",
      "2019-03-26 12:27:49,371 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2019-03-26 12:27:49,372 : INFO : effective_min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2019-03-26 12:27:49,451 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2019-03-26 12:27:49,455 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-03-26 12:27:49,455 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2019-03-26 12:27:49,516 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2019-03-26 12:27:49,517 : INFO : resetting layer weights\n",
      "2019-03-26 12:27:49,834 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-26 12:27:50,843 : INFO : EPOCH 1 - PROGRESS: at 6.51% examples, 826988 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:51,861 : INFO : EPOCH 1 - PROGRESS: at 13.65% examples, 856916 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:52,864 : INFO : EPOCH 1 - PROGRESS: at 20.56% examples, 861118 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:53,865 : INFO : EPOCH 1 - PROGRESS: at 27.69% examples, 872587 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:54,868 : INFO : EPOCH 1 - PROGRESS: at 34.88% examples, 878842 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:55,875 : INFO : EPOCH 1 - PROGRESS: at 42.14% examples, 886296 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:56,882 : INFO : EPOCH 1 - PROGRESS: at 49.33% examples, 890603 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:57,885 : INFO : EPOCH 1 - PROGRESS: at 56.05% examples, 886255 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:58,886 : INFO : EPOCH 1 - PROGRESS: at 63.10% examples, 888571 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:27:59,908 : INFO : EPOCH 1 - PROGRESS: at 70.28% examples, 889306 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:00,909 : INFO : EPOCH 1 - PROGRESS: at 77.60% examples, 893523 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:01,913 : INFO : EPOCH 1 - PROGRESS: at 84.78% examples, 895063 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:02,916 : INFO : EPOCH 1 - PROGRESS: at 91.55% examples, 892604 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:03,918 : INFO : EPOCH 1 - PROGRESS: at 98.82% examples, 895097 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:04,055 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-26 12:28:04,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-26 12:28:04,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-26 12:28:04,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-26 12:28:04,070 : INFO : EPOCH - 1 : training on 17798082 raw words (12749528 effective words) took 14.2s, 896038 effective words/s\n",
      "2019-03-26 12:28:05,085 : INFO : EPOCH 2 - PROGRESS: at 7.24% examples, 914072 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:06,090 : INFO : EPOCH 2 - PROGRESS: at 14.51% examples, 913037 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:07,094 : INFO : EPOCH 2 - PROGRESS: at 21.86% examples, 917027 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:08,098 : INFO : EPOCH 2 - PROGRESS: at 28.70% examples, 904998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:09,101 : INFO : EPOCH 2 - PROGRESS: at 36.06% examples, 909158 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:10,107 : INFO : EPOCH 2 - PROGRESS: at 43.36% examples, 912981 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:11,109 : INFO : EPOCH 2 - PROGRESS: at 50.57% examples, 914054 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:12,115 : INFO : EPOCH 2 - PROGRESS: at 57.73% examples, 914343 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-26 12:28:13,127 : INFO : EPOCH 2 - PROGRESS: at 64.62% examples, 909413 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:14,129 : INFO : EPOCH 2 - PROGRESS: at 71.65% examples, 908363 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:15,141 : INFO : EPOCH 2 - PROGRESS: at 78.79% examples, 907413 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:16,145 : INFO : EPOCH 2 - PROGRESS: at 85.90% examples, 907269 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:17,145 : INFO : EPOCH 2 - PROGRESS: at 93.12% examples, 908396 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:18,073 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-26 12:28:18,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-26 12:28:18,087 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-26 12:28:18,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-26 12:28:18,097 : INFO : EPOCH - 2 : training on 17798082 raw words (12750864 effective words) took 14.0s, 909403 effective words/s\n",
      "2019-03-26 12:28:19,123 : INFO : EPOCH 3 - PROGRESS: at 7.13% examples, 893060 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-26 12:28:20,138 : INFO : EPOCH 3 - PROGRESS: at 14.62% examples, 912158 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-26 12:28:21,148 : INFO : EPOCH 3 - PROGRESS: at 21.98% examples, 914599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:22,154 : INFO : EPOCH 3 - PROGRESS: at 29.36% examples, 920451 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:23,157 : INFO : EPOCH 3 - PROGRESS: at 36.95% examples, 927114 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:24,160 : INFO : EPOCH 3 - PROGRESS: at 43.69% examples, 916472 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:25,176 : INFO : EPOCH 3 - PROGRESS: at 50.95% examples, 916262 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:26,178 : INFO : EPOCH 3 - PROGRESS: at 58.24% examples, 918421 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:27,192 : INFO : EPOCH 3 - PROGRESS: at 65.73% examples, 921428 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:28,192 : INFO : EPOCH 3 - PROGRESS: at 73.06% examples, 923005 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:29,196 : INFO : EPOCH 3 - PROGRESS: at 79.97% examples, 918722 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:30,215 : INFO : EPOCH 3 - PROGRESS: at 87.42% examples, 919976 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-26 12:28:31,218 : INFO : EPOCH 3 - PROGRESS: at 94.60% examples, 919425 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:31,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-26 12:28:31,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-26 12:28:31,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-26 12:28:31,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-26 12:28:31,962 : INFO : EPOCH - 3 : training on 17798082 raw words (12748713 effective words) took 13.9s, 920103 effective words/s\n",
      "2019-03-26 12:28:32,968 : INFO : EPOCH 4 - PROGRESS: at 7.19% examples, 914619 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:33,980 : INFO : EPOCH 4 - PROGRESS: at 14.11% examples, 889251 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-26 12:28:34,987 : INFO : EPOCH 4 - PROGRESS: at 21.12% examples, 886348 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:35,989 : INFO : EPOCH 4 - PROGRESS: at 28.26% examples, 891376 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-26 12:28:36,991 : INFO : EPOCH 4 - PROGRESS: at 35.50% examples, 895508 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:38,001 : INFO : EPOCH 4 - PROGRESS: at 42.57% examples, 896206 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-26 12:28:39,003 : INFO : EPOCH 4 - PROGRESS: at 49.66% examples, 897716 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-26 12:28:40,010 : INFO : EPOCH 4 - PROGRESS: at 56.44% examples, 892876 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:41,015 : INFO : EPOCH 4 - PROGRESS: at 63.67% examples, 896487 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:42,015 : INFO : EPOCH 4 - PROGRESS: at 70.82% examples, 898431 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:43,018 : INFO : EPOCH 4 - PROGRESS: at 78.00% examples, 899734 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:44,024 : INFO : EPOCH 4 - PROGRESS: at 85.28% examples, 901771 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:45,024 : INFO : EPOCH 4 - PROGRESS: at 91.98% examples, 898420 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:46,030 : INFO : EPOCH 4 - PROGRESS: at 99.16% examples, 899272 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:46,129 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-26 12:28:46,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-26 12:28:46,132 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-26 12:28:46,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-26 12:28:46,154 : INFO : EPOCH - 4 : training on 17798082 raw words (12751690 effective words) took 14.2s, 898917 effective words/s\n",
      "2019-03-26 12:28:47,164 : INFO : EPOCH 5 - PROGRESS: at 7.13% examples, 903952 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:48,166 : INFO : EPOCH 5 - PROGRESS: at 14.45% examples, 912707 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:49,173 : INFO : EPOCH 5 - PROGRESS: at 21.97% examples, 923234 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:50,173 : INFO : EPOCH 5 - PROGRESS: at 28.93% examples, 913911 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:51,185 : INFO : EPOCH 5 - PROGRESS: at 36.29% examples, 914977 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:52,205 : INFO : EPOCH 5 - PROGRESS: at 43.52% examples, 914588 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:53,222 : INFO : EPOCH 5 - PROGRESS: at 50.85% examples, 915590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:54,223 : INFO : EPOCH 5 - PROGRESS: at 58.01% examples, 916245 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:55,230 : INFO : EPOCH 5 - PROGRESS: at 64.95% examples, 912243 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:56,231 : INFO : EPOCH 5 - PROGRESS: at 72.15% examples, 913248 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:57,233 : INFO : EPOCH 5 - PROGRESS: at 79.52% examples, 915154 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:28:58,240 : INFO : EPOCH 5 - PROGRESS: at 86.76% examples, 915261 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-26 12:28:59,251 : INFO : EPOCH 5 - PROGRESS: at 94.15% examples, 916690 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-26 12:29:00,087 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-26 12:29:00,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-26 12:29:00,099 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-26 12:29:00,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-26 12:29:00,106 : INFO : EPOCH - 5 : training on 17798082 raw words (12751125 effective words) took 13.9s, 914327 effective words/s\n",
      "2019-03-26 12:29:00,106 : INFO : training on a 88990410 raw words (63751920 effective words) took 70.3s, 907236 effective words/s\n",
      "2019-03-26 12:29:00,107 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-03-26 12:29:00,366 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-03-26 12:29:00,367 : INFO : not storing attribute vectors_norm\n",
      "2019-03-26 12:29:00,369 : INFO : not storing attribute cum_table\n",
      "2019-03-26 12:29:01,020 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers,\n",
    "                         size = num_features,\n",
    "                         min_count = min_word_count,\n",
    "                         window = context,\n",
    "                         sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장에 어울리지 않는 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py:893: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지정 단어에 가장 근접한 단어들 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6287728548049927),\n",
       " ('lady', 0.5718315839767456),\n",
       " ('lad', 0.5706989765167236),\n",
       " ('farmer', 0.5474367737770081),\n",
       " ('men', 0.5279064178466797),\n",
       " ('chap', 0.5121819972991943),\n",
       " ('monk', 0.5119988918304443),\n",
       " ('politician', 0.5100113749504089),\n",
       " ('guy', 0.5092923045158386),\n",
       " ('person', 0.5090522766113281)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6503031253814697),\n",
       " ('bride', 0.616742730140686),\n",
       " ('goddess', 0.6125830411911011),\n",
       " ('victoria', 0.6076536178588867),\n",
       " ('belle', 0.5995310544967651),\n",
       " ('marlene', 0.579119086265564),\n",
       " ('latifah', 0.5784250497817993),\n",
       " ('maid', 0.5770208239555359),\n",
       " ('mistress', 0.5641007423400879),\n",
       " ('showgirl', 0.5595673322677612)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7574539184570312),\n",
       " ('horrible', 0.7391939759254456),\n",
       " ('dreadful', 0.7299064993858337),\n",
       " ('atrocious', 0.7201664447784424),\n",
       " ('abysmal', 0.7008265256881714),\n",
       " ('appalling', 0.6705971956253052),\n",
       " ('horrid', 0.6693190336227417),\n",
       " ('horrendous', 0.6669641137123108),\n",
       " ('lousy', 0.6343021392822266),\n",
       " ('laughable', 0.6074416637420654)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
