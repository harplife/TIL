{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec을 활용한 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/bow/labeledTrainData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"./data/bow/testData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"./data/bow/unlabeledTrainData.tsv\", \n",
    "                    sep=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 모델로 Feature들 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all of the word vectors in a given paragraph.\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # 1. pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    \n",
    "    # 2. index2word contains model's vocabulary.\n",
    "    # Convert to set, for speed\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # 3. loop over each word in the review,\n",
    "    # if it is in the model's vocab,\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # 4. divide the result by the number of words\n",
    "    # to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of reviews (each one a list of words),\n",
    "# calculate the average feature vector for each one\n",
    "# and return a 2 dimensional numpy array\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # 1. initialize a counter\n",
    "    counter = 0\n",
    "    # 2. preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),\n",
    "                                  num_features),\n",
    "                                 dtype=\"float32\")\n",
    "    # 3. loop through the reviews\n",
    "    for review in reviews:\n",
    "        # a. print status at every 1000th review\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        # b. call the function that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review,\n",
    "                                                    model,\n",
    "                                                    num_features)\n",
    "        # c. increment the counter\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 1. remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. convert words to lower case and split\n",
    "    words = review_text.lower().split()\n",
    "    # 4. optional: remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # 5. return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터(특징) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300 # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(\n",
    "        review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, \n",
    "                                  model, \n",
    "                                  num_features)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "clean_test_reviews = []\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(\n",
    "        review_to_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, \n",
    "                                 model, \n",
    "                                 num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# using random forest to train a model\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "\n",
    "output.to_csv(\"W2V_AvgVec.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering: 434.6488604545593 seconds\n"
     ]
    }
   ],
   "source": [
    "# using clustering (k-means) to train a model\n",
    "start = time.time()\n",
    "\n",
    "# 1. set \"k\", number of clusters\n",
    "# 1/5th of the vocab size, or an average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0]/5)\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: {} seconds\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word / index dictionary,\n",
    "# mapping each vocab word to a cluster number\n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['caesar', 'avery', 'hoskins', 'thornton', 'newhart', 'saget', 'cosby', 'bewitched', 'briggs']\n",
      "Cluster 1\n",
      "['whip', 'rusty', 'frog', 'roaring', 'sheets', 'crushing', 'slug', 'facade', 'veil', 'pencil', 'elmo', 'cement', 'bolt', 'pancake', 'bubbles', 'roar', 'cleaver', 'booty', 'wool', 'puddle', 'sweating', 'hose', 'frying']\n",
      "Cluster 2\n",
      "['divide']\n",
      "Cluster 3\n",
      "['messages', 'symbols', 'meanings', 'metaphors', 'mechanics']\n",
      "Cluster 4\n",
      "['rebecca', 'daisy', 'andrea', 'paula', 'cindy', 'esther', 'alicia', 'alison', 'clara', 'lena', 'fiona', 'sadie', 'stacy', 'cecilia', 'teresa', 'erin', 'raines', 'pauline', 'iris', 'lori', 'suzanne', 'cheryl', 'mona', 'constance', 'nell', 'lindsey', 'cassie', 'flora', 'hemingway', 'hazel', 'vicki', 'abigail', 'marjorie', 'vicky', 'jen', 'dominique', 'delilah', 'ella', 'violet', 'blanche', 'hagen', 'lilly', 'margo', 'susie', 'andr', 'sobieski', 'lizzie', 'becky', 'libby', 'yvette', 'alonso', 'bai']\n",
      "Cluster 5\n",
      "['trained', 'protected', 'cured', 'naming', 'awakened', 'contaminated']\n",
      "Cluster 6\n",
      "['overall', 'otherwise']\n",
      "Cluster 7\n",
      "['circuit', 'venice', 'edinburgh', 'attendance']\n",
      "Cluster 8\n",
      "['frantic', 'hectic', 'swiftly']\n",
      "Cluster 9\n",
      "['wielding', 'tomatoes', 'bees', 'scarecrows', 'behemoth', 'shrews', 'booby', 'cockroaches', 'ghouls', 'ghoul', 'wasps']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print(\"Cluster {}\".format(cluster))\n",
    "    words = []\n",
    "    for i in range(0, len(word_centroid_map.values())):\n",
    "        if (list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "train_centroids = np.zeros(\n",
    "    (train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = \\\n",
    "        create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "        \n",
    "test_centroids = np.zeros(\n",
    "    (test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] =\\\n",
    "        create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "        \n",
    "        \n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"],\n",
    "                           \"sentiment\":result})\n",
    "\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
