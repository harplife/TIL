{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensorflow로 배우는<span class=\"tocSkip\"></span></h3>\n",
    "<h1>Word2Vec<span class=\"tocSkip\"></span></h1>\n",
    "<h2>(Skip-Gram model)<span class=\"tocSkip\"></span></h2>\n",
    "\n",
    "<br>\n",
    "리소스: https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초간단 Knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 개요\n",
    "    1. Word Embedding\n",
    "    2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "    word embedding (단어 임베딩)이란 텍스트를 구성하는\n",
    "    각 단어를 (주로 머신러닝을 위해) 수치화(벡터화)하는 작업이다.\n",
    "    이에 기본적인 예는 BoW (Bag of Words)이다.\n",
    "    여기서 이어 CBBOW (Continuous bow),\n",
    "    Skip-Gram,\n",
    "    그리고 CBOW, Skip-Gram 짬뽕:\n",
    "    gensim 패키지에 속한 Word2Vec이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "    - 위에 말했듯이 Word2Vec은 CBOW와 Skim-Gram에 짬뽕이다.\n",
    "    - 이에 여러 작업이 추가되어 있다.\n",
    "        a. hierarchial softmax\n",
    "        b. negative sampling\n",
    "    - Word2Vec은 구글사에서 제공되며,\n",
    "    - 오픈소스여서 누구나 사용할 수 있다.\n",
    "    - 2013년도에 Tomas Mikolov(와 다른 팀 멤버들)이 만들었다.\n",
    "        - 논문: https://arxiv.org/pdf/1310.4546.pdf\n",
    "    - shallow neural network이며, 3개 레이어를 사용한다.\n",
    "        a. 1 input layer\n",
    "        b. 1 hidden layer\n",
    "        c. 1 output layer\n",
    "    - word2vec의 특징:\n",
    "        a. 지정 단어에 연관된 단어를 예측하는 식으로 학습\n",
    "        b. output layer를 제거할 수 있음\n",
    "        c. hidden layer로 지정 단어의 context를 뽑을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코딩!\n",
    "    원래 Word2Vec은 Gensim library에서 호출해서\n",
    "    아주 쉽게 사용할 수 있다.\n",
    "    그러나 Word2Vec 모델의 원리를 알아낼려면\n",
    "    간단히 코드를 사용해봤자 이해하기 어려우니,\n",
    "    tensorflow를 사용하여 word2vec 기능들을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 library들 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = \"He is the king . The king is royal . She is the royal queen\"\n",
    "\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the king . the king is royal . she is the royal queen\n"
     ]
    }
   ],
   "source": [
    "print(corpus_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for word in corpus_raw.split():\n",
    "    # 단어들만 추출\n",
    "    if word != '.':\n",
    "        words.append(word)\n",
    "        \n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정제된 문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sents = corpus_raw.split('.')\n",
    "\n",
    "sents = []\n",
    "\n",
    "for sent in raw_sents:\n",
    "    sents.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram 모델\n",
    "    Skip-Gram 모델은 input 데이터가 (x, y)형태로,\n",
    "    단어와 예측할 단어 pair(커플)로 학습한다.\n",
    "    이러한 input 데이터가 만들어 지기 위해선,\n",
    "    문장에서 지정 window (단어의 n개 앞,뒤) 사이즈로\n",
    "    문장 단어 하나씩 그 주변의 단어들과 pairing 해준다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram](https://user-images.githubusercontent.com/44990492/56113548-295b6000-5f99-11e9-93f0-a953eb67a797.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# 여기선 단어 앞뒤 2개씩, min 2개 pair, max 4개 pair가 된다.\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sent in sents:\n",
    "    for word_index, word in enumerate(sent):\n",
    "        for nb_word in sent[\\\n",
    "            max(word_index - WINDOW_SIZE, 0) : \\\n",
    "            min(word_index + WINDOW_SIZE, len(sent)) + 1] :\n",
    "                if nb_word != word:\n",
    "                    data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king'], ['the', 'he'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['the', 'king'], ['the', 'is'], ['king', 'the'], ['king', 'is'], ['king', 'royal'], ['is', 'the'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['she', 'is'], ['she', 'the'], ['is', 'she'], ['is', 'the'], ['is', 'royal'], ['the', 'she'], ['the', 'is'], ['the', 'royal'], ['the', 'queen'], ['royal', 'is'], ['royal', 'the'], ['royal', 'queen'], ['queen', 'the'], ['queen', 'royal']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 numbering\n",
    "    머신러닝 학습을 위해 단어를 숫자로 대체해야 된다.\n",
    "    단어에 대한 인덱싱 처리한거라 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'king': 1, 'she': 2, 'the': 3, 'queen': 4, 'royal': 5, 'he': 6}\n",
      "{0: 'is', 1: 'king', 2: 'she', 3: 'the', 4: 'queen', 5: 'royal', 6: 'he'}\n"
     ]
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "print(word2int)\n",
    "print(int2word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-to-N (one-hot) encoding\n",
    "![identity](https://user-images.githubusercontent.com/44990492/56113597-4728c500-5f99-11e9-99ca-e1c66875ab2a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look-up-table\n",
    "    one-hot vector는 identity matrix의 역활을 한다.\n",
    "    즉, 지정 매트릭스에 n 번째 행을 갖고옴으로,\n",
    "    Neural Network에선 Weight 매트릭스에 n 번째\n",
    "    행을 갖고 오는 역활을 한다.\n",
    "    좀 더 정확히 말하자면,\n",
    "    'he'라는 단어가 현재 0000001 로 벡터화 되어있다면,\n",
    "    이를 학습된 Weight1(hidden layer)에 곱하면,\n",
    "    Weight에 7 번째 줄이 뽑힌다.\n",
    "    Weight[7] 줄은 'he'에 대한 단어의\n",
    "    neighbor (context)들에 대한 거리들이 표현되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot처리 동시에 x, y 데이터 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append( to_one_hot( word2int[ data_word[0] ],\n",
    "                              vocab_size))\n",
    "    y_train.append( to_one_hot( word2int[ data_word[1] ],\n",
    "                              vocab_size))\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터에 대한 정의\n",
    "    보다시피, 'he'라는 단어가 [0,0,0,0,0,0,1]로 처리됬고,\n",
    "    'he'의 동네친구 'is'와 'the'는 각 각\n",
    "    0010000, 0000100 으로 바뀌었다.\n",
    "    한 마디로,\n",
    "    0000001 이란 데이터가 들어오면,\n",
    "    0010000, 또는 0000100 이란 데이란 데이터가 예측되어야 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 3.305218458175659\n",
      "loss is : 1.3281892538070679\n",
      "loss is : 1.3235028982162476\n",
      "loss is : 1.3222616910934448\n",
      "loss is : 1.321708083152771\n",
      "loss is : 1.3213995695114136\n",
      "loss is : 1.321204423904419\n",
      "loss is : 1.3210707902908325\n",
      "loss is : 1.3209737539291382\n",
      "loss is : 1.3209004402160645\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 1. INPUT LAYER\n",
    "X = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "Y = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "\n",
    "# 2. HIDDEN LAYER\n",
    "EMBEDDING_DIM = 5\n",
    "# Embedding dimension:\n",
    "# 제대로된 의미는 모르겠지만\n",
    "# 대강 각 단어의 특징을 5개 정도 잡는다고 생각하면 될듯.\n",
    "# 너무 적으면 안 되고 너무 많으면 안된다.\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "\n",
    "# Hidden Representation\n",
    "# 이 모델에 사용되는 Hypothesis는\n",
    "# 따로 사용될 수 있는 \n",
    "# 숨어 있는 용도가 있다.\n",
    "H = tf.add(tf.matmul(X, W1), b1)\n",
    "\n",
    "# 3. OUTPUT LAYER\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "# activation function => softmax 사용\n",
    "prediction = tf.nn.softmax(tf.add(tf.matmul(H, W2), b2))\n",
    "\n",
    "# input_one_hot => embedded repr. => predicted_neighbor_prob\n",
    "# predicted_prob will be compared against a one hot vector\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# cost function => cross entropy (log loss)\n",
    "cross_entropy_loss = tf.reduce_mean(\n",
    "    -tf.reduce_sum(Y * tf.log(prediction), \n",
    "                  reduction_indices=[1]))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "\n",
    "model_train = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "# 학습 반복수\n",
    "epoch = 10000\n",
    "\n",
    "for step in range(epoch):\n",
    "    sess.run(model_train, feed_dict={X: x_train, Y: y_train})\n",
    "    \n",
    "    if step%1000==0:\n",
    "        loss = sess.run(cross_entropy_loss, \n",
    "                        feed_dict={X: x_train, Y: y_train})\n",
    "        print(\"loss is : {}\".format(loss) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Representation\n",
    "    이 모델 자체는 지정 단어에 n-gram 근접한 단어를\n",
    "    예측하는 식으로 학습이 되어 있지만,\n",
    "    이 예측모델을 사용하는 대신에\n",
    "    학습하면서 생기는 첫 번쨰 layer의\n",
    "    W1와 b1 값이다.\n",
    "    W1,b1은 위에 one-hot 처리 사용 (look-up-table)을 사용하여\n",
    "    지정 단어에 대한 학습된 벡터를 찾을 수 가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16884391  0.47522554  1.7791299   2.1045978   1.7259084 ]\n",
      " [ 1.3524605  -0.9047998   0.16236609 -0.30971235 -0.39247093]\n",
      " [-0.47699308 -0.194042   -0.43534783 -1.0635788  -0.09963643]\n",
      " [ 1.6984836   1.1369572  -1.6472584   1.4050508   0.9246166 ]\n",
      " [ 1.2601703  -1.5514985   1.6991419   0.49853742  0.26527128]\n",
      " [-0.10860179  2.4968026  -0.12641595 -0.6618412   0.5468745 ]\n",
      " [ 0.3452465  -0.30957833 -0.03553431 -1.8872234  -0.08141606]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2462664   0.6626038   0.87485343 -0.04988753 -2.7971547 ]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4151102   1.1378293   2.6539834   2.0547104  -1.0712463 ]\n",
      " [ 2.5987267  -0.24219602  1.0372195  -0.3595999  -3.1896255 ]\n",
      " [ 0.7692733   0.4685618   0.4395056  -1.1134664  -2.896791  ]\n",
      " [ 2.9447498   1.799561   -0.77240497  1.3551632  -1.8725381 ]\n",
      " [ 2.5064368  -0.88889474  2.5739954   0.44864988 -2.5318835 ]\n",
      " [ 1.1376646   3.1594064   0.74843746 -0.71172875 -2.2502801 ]\n",
      " [ 1.5915129   0.35302547  0.8393191  -1.9371109  -2.8785708 ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5064368  -0.88889474  2.5739954   0.44864988 -2.5318835 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[ word2int['queen'] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 근접 단어 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance (유클리드 거리)\n",
    "    피타고라스 정리를 기반으로한 유클리드 거리이다.\n",
    "    두 점 사이의 거리를 계산할 때 흔히 사용되는 방법이다.\n",
    "    특히 자연어처리에 있어 두 벡터 사이의 거리를 계산,\n",
    "    즉, 지정 단어와 근접한 단어의 거리를 계산하는데\n",
    "    자주 사용된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유클리드 거리 그림\n",
    "![Euclidean_distance](https://user-images.githubusercontent.com/44990492/56113606-527bf080-5f99-11e9-8f24-4bcf05b34f06.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유클리드 거리 수식\n",
    "![Euclidean_distance_function](https://user-images.githubusercontent.com/44990492/56113615-5ad42b80-5f99-11e9-9075-e0c5dc66ee46.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy로 수식 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    \n",
    "    return np.sqrt(np.sum(np.square(vec1-vec2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 근접 단어 검색 함수 코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity?\n",
    "    min_index = -1\n",
    "    \n",
    "    # input 단어에 대한 벡터값\n",
    "    query_vector = vectors[word_index]\n",
    "    \n",
    "    # 각 벡터에 대한 index와 값들을 갖고와서,\n",
    "    for index, vector in enumerate(vectors):\n",
    "        # input단어 벡터와 각 벡터와의 거리가,\n",
    "        # (근접거리:\"무한\"보다는 적고,)\n",
    "        # 전에 적용된 근접거리 보다 적고,\n",
    "        # 두 벡터가 똑같은게 아니면,\n",
    "        if euclidean_dist(vector, query_vector) < min_dist \\\n",
    "        and not np.array_equal(vector, query_vector):\n",
    "            # min_dist (근접거리) 새로 적용\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            # 현재 최소근접단어에 대한 인덱스 적용\n",
    "            min_index = index\n",
    "            \n",
    "    # 위에 방식으로 하나하나 근접거리를 비교하고 나면\n",
    "    # 최소근접거리의 단어 인덱스가 나온다.\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습한 결과\n",
    "    밑에 보다시피 위 학습모델은\n",
    "    king은 queen과 근접하고,\n",
    "    queen은 king과 근접하고,\n",
    "    she는 he와 근접하다는 것을 배웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is queen\n",
      "king queen\n",
      "she he\n",
      "the royal\n",
      "queen king\n",
      "royal she\n",
      "he she\n"
     ]
    }
   ],
   "source": [
    "for w in words:    \n",
    "    print( w, int2word[ find_closest( word2int[w], vectors ) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프로 확인해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디멘션 줄여주는 TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors_2d = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터값 Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer()\n",
    "vectors_2d = normalizer.fit_transform(vectors, '12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib으로 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is: (0.3570253849029541,0.28706875443458557)\n",
      "is  =>  queen\n",
      "\n",
      "king: (0.6093035340309143,-0.056785840541124344)\n",
      "king  =>  queen\n",
      "\n",
      "she: (0.23588350415229797,0.14367586374282837)\n",
      "she  =>  he\n",
      "\n",
      "the: (0.6970052123069763,0.4259456396102905)\n",
      "the  =>  royal\n",
      "\n",
      "queen: (0.5561690330505371,-0.19724243879318237)\n",
      "queen  =>  king\n",
      "\n",
      "royal: (0.27268245816230774,0.7572659850120544)\n",
      "royal  =>  she\n",
      "\n",
      "he: (0.4055476188659668,0.0899575725197792)\n",
      "he  =>  she\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "for w in words:\n",
    "    print(\"{}: ({},{})\".format(w, \n",
    "                               vectors_2d[word2int[w]][0], \n",
    "                               vectors_2d[word2int[w]][1] ))\n",
    "    \n",
    "    print( w, \" => \",int2word[ find_closest( word2int[w], vectors_2d ) ] )\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    ax.annotate(w, \n",
    "        (vectors_2d[word2int[w]][0],vectors_2d[word2int[w]][1]) )\n",
    "    \n",
    "ax.set_ylim(-4,4)\n",
    "ax.set_xlim(-4,4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추가 노트\n",
    "    밑에 보다시피 똑같은 모델을 여러번 돌려본 결과,\n",
    "    학습이 일정하게 되어 있지 않다.\n",
    "    물론 Weight와 bias값이 현재 random_normal로 되어있어\n",
    "    이러한 현상은 자주 볼 수 있다.\n",
    "    하지만 word2vec 모델 자체가 원래\n",
    "    학습데이터가 많아야 학습이 잘 되는 모델이다.\n",
    "    25,000 문장의 샘플 데이터를 사용했어도,\n",
    "    기본 BoW 모델보다 정확성이 훨씬 높거나 훨씬 낮지도 않다.\n",
    "    여기에 이어 negative sampling, hierarchial softmax도\n",
    "    적용해야 그나마 제대로된 짜가 word2vec이 나온다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_1 결과 그래프\n",
    "![model_1](https://user-images.githubusercontent.com/44990492/56113175-0a100300-5f98-11e9-9d1c-ea6849a5cb5d.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_2 결과 그래프\n",
    "![model_2](https://user-images.githubusercontent.com/44990492/56113657-6f182880-5f99-11e9-93d1-8ed89c0c7970.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_3 결과 그래프\n",
    "![model_3](https://user-images.githubusercontent.com/44990492/56113664-74757300-5f99-11e9-837b-4c9f617f28bc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "    Word2Vec은 원래 Skip-Gram (중심단어로 주변단어 찾기) 모델과,\n",
    "    CBOW (주변단어로 중심단어 찾기) 모델로 구분된다.\n",
    "    CBOW 모델로 구현하기 위해선,\n",
    "    현재 학습된 모델에서 input을 주변단어로 넣으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_center(context):\n",
    "    context_hotone = np.zeros(shape=(1,vocab_size))\n",
    "    # 총 단어수 7개보다 적으면,\n",
    "    if len(context) < vocab_size:\n",
    "        # context에 있는 단어가\n",
    "        for word in context:\n",
    "            # vocab에 있는 단어가 맞다면,\n",
    "            if word in words:\n",
    "                # 단어에 대한 hotone encoding을 추출하고\n",
    "                word_hotone = np.unique(x_train, axis=0)[::-1][word2int[word]]\n",
    "                # 단어 hotone encoding들을 합친다\n",
    "                context_hotone = context_hotone + word_hotone\n",
    "            else:\n",
    "                print(\"Word is not in vocabulary\")\n",
    "                return\n",
    "    else:\n",
    "        print(\"Context exceeds the size of the vocabulary\")\n",
    "        return\n",
    "    \n",
    "    # 합쳐진 context hotone를 사용해서 예측\n",
    "    # 원래 모델 자체가 단어 주변에 있는 단어를 예측하는 모델이니,\n",
    "    # 여러 단어들에 공통적으로 주변에 있는 단어를 찾게 된다.\n",
    "    probs = sess.run(prediction, feed_dict={X: context_hotone})\n",
    "    # 예측할 단어 리스트에서 context 단어를 제외\n",
    "    for word in context:\n",
    "        if word in int2word.values():\n",
    "            # 일단 제외방식을 확률을 -로 바꾸는걸로 한다.\n",
    "            probs[0][word2int[word]] = -probs[0][word2int[word]]\n",
    "    # 확률이 제일 높은거의 인덱스 가져오기\n",
    "    center_idx = np.argmax(probs)\n",
    "    # 인덱스 look up해서 단어 가져오기\n",
    "    result = int2word[center_idx]\n",
    "    \n",
    "    return result, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the king . the king is royal . she is the royal queen'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0, 'king': 1, 'she': 2, 'the': 3, 'queen': 4, 'royal': 5, 'he': 6}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'is', 'the', 'queen'] : royal\n"
     ]
    }
   ],
   "source": [
    "# 위에 문장을 확인해보면 \"she is the royal queen\"이다.\n",
    "# 'royal'을 뺀 문장, 'she is the ___ queen'의 값을 넣어주면\n",
    "# 이 사이의 단어를 알려준다.\n",
    "context = ['she','is','the','queen']\n",
    "result, probs = find_center(context)\n",
    "print(\"{} : {}\".format(context, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00000011,  0.01272334, -0.00247079, -0.0000056 , -0.00000007,\n",
       "         0.5790665 ,  0.4057336 ]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률 벡터를 확인하여 확률이 높은 단어들을 확인해본다.\n",
    "# 보다시피 royal이 제일 확률이 높았고,\n",
    "# 그 다음으로 'he'가 높았다.\n",
    "# 그 이유는 'is','the' 둘다 'he'와 제법 근접한 단어들이기 때문이다.\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is queen\n",
      "king queen\n",
      "she he\n",
      "the royal\n",
      "queen king\n",
      "royal she\n",
      "he she\n"
     ]
    }
   ],
   "source": [
    "for w in words:    \n",
    "    print( w, int2word[ find_closest( word2int[w], vectors ) ] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "733px",
    "left": "998px",
    "top": "94px",
    "width": "256px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
